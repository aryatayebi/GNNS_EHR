{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Differences\n",
    "\n",
    "- transformers instead of transformers_custom\n",
    "- new_xallFile instead of xallFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from tqdm.auto import tqdm\n",
    "from transformers_custom import BertForMaskedLM, BertConfig, AdamW, get_scheduler, BertForTokenClassification, BertTokenizerFast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Read In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10950619, 192330)\n",
      "(10950619, 192329)\n"
     ]
    }
   ],
   "source": [
    "headerFile = \"/gpfs/data/razavianlab/capstone/2021_ehr/preprocessed_headers.pkl\"\n",
    "mrnsFile = \"/gpfs/data/razavianlab/capstone/2021_ehr/mrns_all_deid.pkl\"\n",
    "#xallFile = \"/gpfs/data/razavianlab/capstone/2021_ehr/preprocessed_xall.npz\"\n",
    "new_xallFile = \"/gpfs/data/razavianlab/capstone/2021_ehr/preprocessed_xall_with_outcome.npz\"\n",
    "\n",
    "headers = np.load(headerFile, allow_pickle=True)\n",
    "data = scipy.sparse.load_npz(new_xallFile)\n",
    "\n",
    "print (data.shape)\n",
    "dementia_outcomes = data[:, -1]\n",
    "data = data[:,:(data.shape[1]-1)]\n",
    "print (data.shape)\n",
    "\n",
    "dementia_outcomes = dementia_outcomes.toarray()\n",
    "\n",
    "positive_indices = np.where(dementia_outcomes == 1)[0]\n",
    "negative_indices = np.where(dementia_outcomes == 0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        #return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return {key: (val[idx].clone().detach()) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "\n",
    "def masking(input_ids, maskedTokenID, masked_precentage):\n",
    "    # labelling\n",
    "    inputs = {'input_ids':input_ids}\n",
    "    labels = inputs[\"input_ids\"].detach().clone()\n",
    "\n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs[\"input_ids\"].shape)\n",
    "    \n",
    "    # create mask array\n",
    "    mask_arr = (rand < masked_precentage) * (inputs[\"input_ids\"] != 0) #* (inputs[\"input_ids\"] != 102) * (inputs[\"input_ids\"] != 0)\n",
    "    non_mask_arr = ~mask_arr\n",
    "    selection = []\n",
    "    normal_selection = []\n",
    "    # getting true (masked value) indexes\n",
    "    for i in range(inputs[\"input_ids\"].shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "          )\n",
    "        normal_selection.append(\n",
    "            torch.flatten(non_mask_arr[i].nonzero()).tolist()\n",
    "          )\n",
    "\n",
    "    # giving a new input mask input id of vocabSize\n",
    "    for i in range(inputs[\"input_ids\"].shape[0]):\n",
    "        inputs[\"input_ids\"][i, selection[i]] = maskedTokenID\n",
    "        labels[i,normal_selection[i]] = -100\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def mapping(input_ids_tensor):\n",
    "    \n",
    "    old_max = torch.max(input_ids_tensor).item()\n",
    "    # mapping input ids to lower numbers \n",
    "    #(i.e. if largest input id is 110,000. Then mapped to 99 if max seq length is 100)\n",
    "    dict_keys = torch.unique(input_ids_tensor).tolist()\n",
    "    dict_values = list(range(torch.unique(input_ids_tensor).shape[0]))\n",
    "    d = dict(zip(dict_keys,dict_values))\n",
    "    new_ids_tensor = input_ids_tensor.apply_(d.get)\n",
    "    \n",
    "    \n",
    "    print(\"shifting values from max of: \", old_max, \"to\",dict_values[len(dict_values)-1])\n",
    "    \n",
    "    return new_ids_tensor, d\n",
    "\n",
    "# creating a tensor of padded input id sequences\n",
    "def input_ids_creator(data, MAX_SEQUENCE_LENGTH,MIN_SEQUENCE_LENGTH, N, negative_indices, positive_indices):\n",
    "    final_list = []\n",
    "    lens = []\n",
    "    study_indexes = []\n",
    "\n",
    "    negative_indices_sample = random.sample(list(negative_indices), data.shape[0]//N)\n",
    "    \n",
    "    indices_to_train_on = negative_indices_sample+list(positive_indices)\n",
    "\n",
    "    # getting an input sequence from every 1000 patients\n",
    "    for i in indices_to_train_on:\n",
    "\n",
    "        patient_study_index = i\n",
    "        row = data.indices[data.indptr[patient_study_index]:data.indptr[patient_study_index+1]]\n",
    "\n",
    "\n",
    "        if (len(row)<MAX_SEQUENCE_LENGTH and len(row)>MIN_SEQUENCE_LENGTH):\n",
    "            lens.append(len(row))\n",
    "            final_list.append(torch.tensor(row))\n",
    "            study_indexes.append(patient_study_index)\n",
    "\n",
    "    emb_length = max(lens)\n",
    "    print(\"embedding length\", emb_length)\n",
    "    print(\"number of patients\", len(final_list), \"out of a possible\", str(data.shape[0]//N), \"iterated on\") \n",
    "    \n",
    "    # stacking sequences and padding with zeroes\n",
    "    input_ids_tensor = torch.stack([torch.cat([i, i.new_zeros(emb_length - i.size(0))], 0) for i in final_list],0)\n",
    "    \n",
    "    return input_ids_tensor, emb_length, study_indexes, final_list\n",
    "\n",
    "def get_train_val_indexes(input_ids_tensor, precentage):\n",
    "    indexes = list(range(input_ids_tensor.shape[0]))\n",
    "    random.shuffle(indexes)\n",
    "    max_train_ind = int(len(indexes)*.8)\n",
    "    train_indexes = indexes[:max_train_ind]\n",
    "    val_indexes = indexes[max_train_ind:len(indexes)]\n",
    "    \n",
    "    return train_indexes, val_indexes\n",
    "\n",
    "def train_val_split(inputs, trainIDs, valIDs):\n",
    "    \n",
    "    train = {}\n",
    "    val = {}\n",
    "    \n",
    "    train[\"input_ids\"] = inputs['input_ids'][trainIDs,:]\n",
    "    train[\"labels\"] = inputs['labels'][trainIDs,:]\n",
    "    \n",
    "    val[\"input_ids\"] = inputs['input_ids'][valIDs,:]\n",
    "    val[\"labels\"] = inputs['labels'][valIDs,:]\n",
    "    \n",
    "    return train,val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length 99\n",
      "number of patients 1799820 out of a possible 2190123 iterated on\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100 # max number of features\n",
    "MIN_SEQUENCE_LENGTH = 0\n",
    "N = 5 # divides dataset by N and loop through every patient\n",
    "\n",
    "# creating a tensor of padded input id sequences\n",
    "input_ids_tensor, emb_length, study_indexes, final_list  = input_ids_creator(data, MAX_SEQUENCE_LENGTH,MIN_SEQUENCE_LENGTH, N, negative_indices, positive_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train, validation indexes\n",
    "# .8 is the train/val split\n",
    "trainIDs, valIDs = get_train_val_indexes(input_ids_tensor, .8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping and Masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shifting values from max of:  192328 to 59840\n"
     ]
    }
   ],
   "source": [
    "# maps feature ids to a lower integers to space model space\n",
    "input_ids_tensor, mapping_dict = mapping(input_ids_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maskedTokenID 59841\n"
     ]
    }
   ],
   "source": [
    "masked_precentage = .3\n",
    "\n",
    "# grabbing largest token ID\n",
    "vocabSize = torch.max(input_ids_tensor).item()\n",
    "# creating maskedToken\n",
    "dimentia = vocabSize\n",
    "maskedTokenID = vocabSize + 1\n",
    "\n",
    "# creating labels and masking select input ids\n",
    "inputs = masking(input_ids_tensor, maskedTokenID, masked_precentage)\n",
    "\n",
    "print(\"maskedTokenID\", maskedTokenID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, val_inputs = train_val_split(inputs, trainIDs, valIDs)\n",
    "train_dataset = OurDataset(train_inputs)\n",
    "val_dataset = OurDataset(val_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending model to  cuda\n"
     ]
    }
   ],
   "source": [
    "# config \n",
    "config = BertConfig(\n",
    "    vocab_size=vocabSize+2,\n",
    "    max_position_embeddings=emb_length,\n",
    "    intermediate_size=3072,\n",
    "    hidden_size=512,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=8,\n",
    "    #type_vocab_size=5,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    #num_labels=3,\n",
    "    position_embedding_type = None\n",
    ")\n",
    "\n",
    "# MLM\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Steps and Epochs\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print (len(train_dataset), len(val_dataset))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(\"sending model to \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(59842, 512, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=512, out_features=59842, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single GPU\n",
    "# model = BertForMaskedLM(config)\n",
    "# print(\"sending model to \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab8d0f45ec64279afcba3d89d035409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3470538854598999\n",
      "val iteration 0\n",
      "tensor(119747.5625, device='cuda:0') 359964 tensor(0.3327, device='cuda:0')\n",
      "validation loss: tensor(0.3327, device='cuda:0')\n",
      "50 0.17420728504657745\n",
      "100 0.16393838822841644\n",
      "150 0.2005435675382614\n",
      "200 0.23440062999725342\n",
      "250 0.19395661354064941\n",
      "300 0.22425399720668793\n",
      "350 0.18233589828014374\n",
      "400 0.16007289290428162\n",
      "450 0.21747060120105743\n",
      "500 0.2001754194498062\n",
      "val iteration 500\n",
      "tensor(66478.0391, device='cuda:0') 359964 tensor(0.1847, device='cuda:0')\n",
      "validation loss: tensor(0.1847, device='cuda:0')\n",
      "550 0.17144235968589783\n",
      "600 0.14796429872512817\n",
      "650 0.17956772446632385\n",
      "700 0.2062743902206421\n",
      "750 0.22472381591796875\n",
      "800 0.1879940927028656\n",
      "850 0.17452885210514069\n",
      "900 0.20086173713207245\n",
      "950 0.16901443898677826\n",
      "1000 0.22801579535007477\n",
      "val iteration 1000\n"
     ]
    }
   ],
   "source": [
    "# model.train()\n",
    "\n",
    "file = open(\"epochs.txt\",\"w+\")\n",
    "file = open(\"epochs_val.txt\", \"w+\")\n",
    "\n",
    "train_losses = []\n",
    "train_iterations = []\n",
    "\n",
    "validation_losses = []\n",
    "validation_iterations = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  \n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    iteration = 0\n",
    "    \n",
    "    total_validation_loss = 0.0\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "\n",
    "    for batch in loop:\n",
    "\n",
    "        outputs = model(input_ids = batch[\"input_ids\"].long().to(device), labels=batch[\"labels\"].long().to(device), return_dict=True)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # saving model\n",
    "        if (iteration%50 == 0 or iteration == 0):\n",
    "            train_iterations.append(iteration)\n",
    "            train_losses.append(loss.item()/BATCH_SIZE)\n",
    "            print (iteration, loss.item()/BATCH_SIZE)\n",
    "            file = open(\"epochs.txt\", \"a\")\n",
    "            file.write(str(epoch) + \" at iteration \" + str(iteration) + \"\\n\")\n",
    "            file.write(\"loss \" + str(loss.item()/BATCH_SIZE) + \"\\n\")\n",
    "            file.close()\n",
    "            torch.save(model, 'model.pkl')\n",
    "            \n",
    "            trained_embedding_matrix = model.bert.embeddings.word_embeddings.weight.data\n",
    "            torch.save(trained_embedding_matrix, 'trained_embedding_matrix.pkl')\n",
    "\n",
    "        # step, zero grad\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "       \n",
    "        # validation set\n",
    "        if (iteration%500 == 0 or iteration == 0):\n",
    "            print (\"val iteration\", iteration)\n",
    "            validation_iterations.append(iteration)\n",
    "            val_iteration = 0\n",
    "            total_validation_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_dataloader:\n",
    "                    val_iteration+=1\n",
    "                    outputs = model(input_ids = val_batch[\"input_ids\"].long().to(device), labels=val_batch[\"labels\"].long().to(device), return_dict=True)\n",
    "                    total_validation_loss += outputs.loss.detach()\n",
    "            mean_validation_loss = total_validation_loss / len(val_dataset)\n",
    "            print (total_validation_loss, len(val_dataset), mean_validation_loss)\n",
    "            validation_losses.append(mean_validation_loss.item())\n",
    "            print (\"validation loss:\", mean_validation_loss)\n",
    "            file = open(\"epochs_val.txt\", \"a\")\n",
    "            #file.write(str(epoch) + \" at iteration \" + str(iteration) + \"\\n\")\n",
    "            file.write(\"loss \" + str(mean_validation_loss) + \"\\n\")\n",
    "            file.write(\"iteration \" + str(iteration) + \"\\n\")\n",
    "            file.close()              \n",
    "        iteration += 1         \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving iteration losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_iterations.pkl','wb') as f:\n",
    "    pickle.dump(train_iterations, f)\n",
    "    \n",
    "with open('train_losses.pkl','wb') as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "    \n",
    "with open('validation_iterations.pkl','wb') as f:\n",
    "    pickle.dump(validation_iterations, f)\n",
    "    \n",
    "with open('validation_losses.pkl','wb') as f:\n",
    "    pickle.dump(validation_losses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(validation_iterations, validation_losses)\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(train_iterations, train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPU Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel import DataParallelModel, DataParallelCriterion, gather\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple GPUs\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "#model= nn.DataParallel(model, device_ids=[0, 1, 2, 3, 4, 5, 6, 7],output_device=[0])\n",
    "\n",
    "model= DataParallelModel(model)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(\"sending model to \", device)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, pin_memory=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "file = open(\"epochs.txt\",\"w+\")\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  \n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    iteration = 0\n",
    "    \n",
    "    total_validation_loss = 0.0\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "\n",
    "    for batch in loop:\n",
    "\n",
    "    # forward, loss, backprop\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].cuda()\n",
    "        batch[\"labels\"] = batch[\"labels\"].cuda()\n",
    "        outputs = model(input_ids = batch[\"input_ids\"].long().cuda(), labels=batch[\"labels\"].long().cuda(), return_dict=True)\n",
    "        outputs = gather(outputs, target_device=0)\n",
    "        loss = outputs.loss.mean()\n",
    "#         print (loss)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        print (loss)\n",
    "        # saving model\n",
    "        if (iteration%5 == 0):\n",
    "            file = open(\"epochs.txt\", \"a\")\n",
    "            file.write(str(epoch) + \" at iteration \" + str(iteration) + \"\\n\")\n",
    "            file.write(\"loss \" + str(loss) + \"\\n\")\n",
    "            file.close()\n",
    "            torch.save(model, 'model.pkl')\n",
    "            \n",
    "            # save word embeddings after each check-in\n",
    "            #trained_embedding_matrix = model.bert.embeddings.word_embeddings.weight.data\n",
    "            trained_embedding_matrix = model.module.bert.embeddings.word_embeddings.weight.data\n",
    "            torch.save(trained_embedding_matrix, 'trained_embedding_matrix.pkl')\n",
    "        \n",
    "\n",
    "        # step, zero grad\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        iteration += 1\n",
    "        \n",
    "        # validation set\n",
    "#         with torch.no_grad():\n",
    "#             for val_batch in val_dataloader:\n",
    "#                 outputs = model(input_ids = val_batch[\"input_ids\"].long().to(device), labels=val_batch[\"labels\"].long().to(device), return_dict=True)\n",
    "#                 outputs = gather(outputs, target_device=0)\n",
    "#                 val_loss = outputs.loss.mean()\n",
    "#                 total_validation_loss += val_loss.item()\n",
    "        \n",
    "        del outputs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # loss calculations\n",
    "    mean_train_loss = total_train_loss / len(train_dataset)\n",
    "    mean_validation_loss = total_validation_loss / len(val_dataset)\n",
    "\n",
    "    validation_losses.append(mean_validation_loss)\n",
    "    train_losses.append(mean_train_loss)\n",
    "        \n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=125)\n",
    "\n",
    "#plt.subplot(121)\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(validation_losses, label='validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Losses')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Saving for DownStream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_dict\n",
    "trained_embedding_matrix = model.bert.embeddings.word_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_embedding_matrix, 'trained_embedding_matrix_LARGE.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mapping_dict_LARGE.pkl','wb') as f:\n",
    "    pickle.dump(mapping_dict, f)\n",
    "with open('study_indexes_LARGE.pkl','wb') as f:\n",
    "    pickle.dump(study_indexes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_modified = []\n",
    "\n",
    "for x in final_list:\n",
    "    final_list_modified.append([mapping_dict[i] for i in x.numpy()])\n",
    "\n",
    "with open('final_list_modified_LARGE.pkl','wb') as f:\n",
    "    pickle.dump(final_list_modified, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /gpfs/data/razavianlab/capstone/2021_ehr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_embedding_matrix_LARGE.pkl','wb') as f:\n",
    "    pickle.dump(trained_embedding_matrix, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
